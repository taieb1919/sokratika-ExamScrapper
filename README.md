# DNB Annales Scraper ğŸ“š

Un outil Python simple et efficace pour scraper et tÃ©lÃ©charger les annales du DiplÃ´me National du Brevet (DNB) depuis le site officiel du ministÃ¨re de l'Ã‰ducation nationale franÃ§ais.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ FonctionnalitÃ©s

- âœ… **Scraping avec Selenium** - Navigation automatique dans les pages avec JavaScript
- âœ… **Pagination automatique** - Parcourt toutes les pages
- âœ… **Extraction de liens** `/document/*/download` avec mÃ©tadonnÃ©es
- âœ… **TÃ©lÃ©chargement concurrent** avec gestion de threads
- âœ… **Organisation automatique** par annÃ©e et matiÃ¨re
- âœ… **Gestion d'erreurs robuste** avec retry automatique
- âœ… **Rate limiting** pour respecter le serveur
- âœ… **Logging complet** avec rotation de fichiers
- âœ… **Interface CLI** intuitive
- âœ… **Mode headless** ou visible pour le debugging

## ğŸ“‹ PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- **Google Chrome** (navigateur)
- **ChromeDriver** (compatible avec votre version de Chrome)
- Connexion Internet

### Installation de ChromeDriver

**MÃ©thode 1 - Automatique (recommandÃ©):**
ChromeDriver sera gÃ©rÃ© automatiquement par Selenium 4.15+

**MÃ©thode 2 - Manuelle:**
1. VÃ©rifiez votre version de Chrome : `chrome://version`
2. TÃ©lÃ©chargez ChromeDriver correspondant : https://chromedriver.chromium.org/downloads
3. Ajoutez ChromeDriver au PATH systÃ¨me ou placez-le dans le dossier du projet

## ğŸš€ Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/taieb1919/sokratika-ExamScrapper.git
cd sokratika-ExamScrapper
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**Note**: Selenium sera installÃ© automatiquement. Assurez-vous que Chrome est installÃ© sur votre systÃ¨me.

## ğŸ“– Utilisation

### Interface en ligne de commande (CLI)

Le projet fournit trois commandes principales :

#### 1. Lister les annÃ©es et matiÃ¨res disponibles

```bash
python main.py list

# Optionnel: lister et lancer une validation des valeurs (enums)
python main.py list --validate

# Limiter le nombre de pages parcourues (ex: 2 premiÃ¨res pages)
python main.py list -p 2
```

Affiche toutes les annÃ©es et matiÃ¨res disponibles avec le nombre de fichiers pour chacune.

#### 2. Scraper sans tÃ©lÃ©charger

```bash
# Extraire tous les liens PDF
python main.py scrape

# Sauvegarder les liens dans un fichier
python main.py scrape --output links.txt

# Limiter Ã  N pages (ex: 3)
python main.py scrape -p 3
```

#### 3. TÃ©lÃ©charger les PDFs

```bash
# TÃ©lÃ©charger tous les PDFs
python main.py download

# TÃ©lÃ©charger avec 10 workers concurrents
python main.py download --workers 10

# TÃ©lÃ©charger dans un rÃ©pertoire personnalisÃ©
python main.py download --output-dir ./mes_pdfs

# Forcer le re-tÃ©lÃ©chargement des fichiers existants
python main.py download --force

# DÃ©sactiver l'organisation par annÃ©e/matiÃ¨re
python main.py download --no-organize

# Limiter Ã  2 pages
python main.py download -p 2
```

#### 4. Valider les valeurs scrapÃ©es (enums) et gÃ©nÃ©rer un CSV

```bash
# Lance un parcours complet des pages et vÃ©rifie les colonnes suivantes :
#   - Session, Discipline, SÃ©rie, Localisation
# Produit un fichier validation_report.csv avec colonnes :
#   Colonne | Valeur_DÃ©tectÃ©e | Enum_Value | Status (OK/MISSING)
python main.py validate

# Limiter Ã  2 pages pour la validation
python main.py validate -p 2
```

### Utilisation programmatique

```python
from src.scraper import DNBScraper
from src.downloader import PDFDownloader
from src.parser import MetadataParser

# Initialisation du scraper (headless par dÃ©faut)
scraper = DNBScraper()

# Ou en mode visible pour debugging
# scraper = DNBScraper(headless=False)

# Extraction des liens PDF (toutes les pages automatiquement)
pdf_links = scraper.extract_pdf_links()
print(f"TrouvÃ© {len(pdf_links)} PDFs sur toutes les pages")

# Obtenir le rÃ©sumÃ©
summary = scraper.get_summary_dict()
print(f"AnnÃ©es disponibles: {summary['years']}")
print(f"MatiÃ¨res disponibles: {summary['subjects']}")

# Parser les mÃ©tadonnÃ©es d'un lien
parser = MetadataParser()
for link_data in pdf_links[:5]:  # Premiers 5 liens
    metadata = parser.parse_url(
        link_data['url'], 
        link_data.get('data_atl_name')
    )
    print(f"Fichier: {metadata['filename']}")
    print(f"AnnÃ©e: {metadata['year']}")
    print(f"MatiÃ¨re: {metadata['subject']}")
    print(f"Type: {metadata['document_type']}")
    print()

# TÃ©lÃ©chargement
urls = [link_data['url'] for link_data in pdf_links]
downloader = PDFDownloader(output_dir="data/raw")
results = downloader.batch_download(
    urls=urls,
    max_workers=5
)

print(f"TÃ©lÃ©chargÃ©s: {len(results['successful'])}")
print(f"Ã‰checs: {len(results['failed'])}")
```

## ğŸ—‚ï¸ Structure du projet

```
sokratika-ExamScrapper/
â”‚
â”œâ”€â”€ src/                        # Code source principal
â”‚   â”œâ”€â”€ __init__.py            # Initialisation du package
â”‚   â”œâ”€â”€ scraper.py             # Module de scraping (DNBScraper)
â”‚   â”œâ”€â”€ downloader.py          # Module de tÃ©lÃ©chargement (PDFDownloader)
â”‚   â”œâ”€â”€ parser.py              # Analyseur de mÃ©tadonnÃ©es (MetadataParser)
â”‚   â””â”€â”€ utils.py               # Fonctions utilitaires
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â””â”€â”€ settings.py            # ParamÃ¨tres de configuration
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es tÃ©lÃ©chargÃ©es
â”‚   â”œâ”€â”€ raw/                   # PDFs tÃ©lÃ©chargÃ©s (organisÃ©s par annÃ©e/matiÃ¨re)
â”‚   â””â”€â”€ metadata/              # Fichiers JSON avec mÃ©tadonnÃ©es
â”‚
â”œâ”€â”€ logs/                      # Fichiers de logs
â”‚
â”œâ”€â”€ tests/                     # Tests (vide dans version simplifiÃ©e)
â”‚
â”œâ”€â”€ main.py                    # Point d'entrÃ©e CLI
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ setup.py                   # Configuration du package
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸ“Š Structure HTML du site

Le scraper utilise **Selenium WebDriver** pour gÃ©rer la pagination JavaScript et extraire les liens de toutes les pages.

### Pagination :
- **Boutons de navigation** : Premier, PrÃ©cÃ©dent, [1][2][3]..., Suivant, Dernier
- **Navigation automatique** : Parcourt toutes les pages jusqu'Ã  la derniÃ¨re avec le bouton "Suivant"
- **JavaScript click** : Utilise JavaScript pour Ã©viter les interceptions d'Ã©lÃ©ments

### Structure du tableau :

```html
<table>
  <tbody>
    <tr>
      <td>Session</td>
      <td>Discipline</td>
      <td>SÃ©rie</td>
      <td>Localisation</td>
      <td class="views-field-link">
        <a href="/document/[ID]/download" data-atl-name="24genfrdag1_v11.pdf|63414">
          TÃ©lÃ©charger
        </a>
      </td>
    </tr>
  </tbody>
</table>
```

### Ã‰lÃ©ments clÃ©s :
- **Liens** : Format `/document/[ID]/download` (pas `.pdf`)
- **MÃ©tadonnÃ©es** : Attribut `data-atl-name` contenant `"filename.pdf|file_id"`
- **Colonnes** : Session, Discipline, SÃ©rie, Localisation, Liens
- **Pagination** : GÃ©rÃ©e dynamiquement avec JavaScript

## ğŸ“Š MÃ©tadonnÃ©es extraites

Le parser extrait automatiquement les informations depuis `data-atl-name` et les noms de fichiers :

- **AnnÃ©e** : AnnÃ©e de l'examen (ex: "2024" depuis "24genfrdag1_v11.pdf")
- **MatiÃ¨re** : MathÃ©matiques, FranÃ§ais, Histoire-GÃ©ographie, Sciences, etc.
- **Session** : normale, remplacement
- **SÃ©rie** : gÃ©nÃ©rale, professionnelle
- **Type de document** : sujet ou correction
- **URL** : Lien de tÃ©lÃ©chargement
- **Nom de fichier** : Nom extrait du data-atl-name
- **ID du fichier** : ID numÃ©rique du document

### Exemple de parsing :

```python
# data-atl-name: "24genfrdag1_v11.pdf|63414"
{
    'filename': '24genfrdag1_v11.pdf',
    'file_id': '63414',
    'year': '2024',           # "24" â†’ "2024"
    'subject': 'FranÃ§ais',    # "fr" dÃ©tectÃ©
    'series': 'generale',     # "gen" dÃ©tectÃ©
    'document_type': 'sujet',
    'is_correction': False,
    'url': 'https://eduscol.education.fr/document/123/download'
}
```

## âš™ï¸ Configuration

### Variables d'environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Selenium settings
SELENIUM_HEADLESS=True         # Mode headless (True) ou visible (False)
SELENIUM_TIMEOUT=20            # Timeout pour les Ã©lÃ©ments (secondes)
SELENIUM_PAGE_LOAD_WAIT=2.0    # Attente entre les pages (secondes)

# Rate limiting
REQUEST_DELAY=1.5              # DÃ©lai entre les requÃªtes (secondes)
DOWNLOAD_TIMEOUT=30            # Timeout de tÃ©lÃ©chargement (secondes)
MAX_RETRIES=3                  # Nombre de tentatives en cas d'Ã©chec

# Download settings
MAX_WORKERS=5                  # Nombre de tÃ©lÃ©chargements concurrents
VERIFY_SSL=True                # VÃ©rifier les certificats SSL

# Organization
ORGANIZE_BY_YEAR=True          # Organiser par annÃ©e
ORGANIZE_BY_SUBJECT=True       # Organiser par matiÃ¨re

# Logging
LOG_LEVEL=INFO                 # Niveau de log (DEBUG, INFO, WARNING, ERROR)
```

### Personnalisation dans `config/settings.py`

Vous pouvez modifier les paramÃ¨tres directement dans `config/settings.py` :

- URL de scraping
- Headers HTTP
- Mapping des matiÃ¨res
- Chemins des fichiers

## ğŸ“ Organisation des fichiers

Les PDFs tÃ©lÃ©chargÃ©s sont automatiquement organisÃ©s :

```
data/raw/
â”œâ”€â”€ 2024/
â”‚   â”œâ”€â”€ FranÃ§ais/
â”‚   â”‚   â”œâ”€â”€ 24genfrdag1_v11.pdf
â”‚   â”‚   â””â”€â”€ 24genfrdag1_corrige.pdf
â”‚   â”œâ”€â”€ MathÃ©matiques/
â”‚   â”‚   â”œâ”€â”€ 24genmathag1_v11.pdf
â”‚   â”‚   â””â”€â”€ 24genmathag1_corrige.pdf
â”‚   â””â”€â”€ Histoire-GÃ©ographie/
â”‚       â””â”€â”€ 24genhisgeoag1_v11.pdf
â””â”€â”€ 2023/
    â””â”€â”€ ...
```

## ğŸ›¡ï¸ Bonnes pratiques implÃ©mentÃ©es

### Respect du serveur
- Rate limiting (dÃ©lai configurable entre requÃªtes)
- User-Agent appropriÃ©
- Gestion des erreurs HTTP
- Retry avec backoff exponentiel

### QualitÃ© du code
- Type hints (PEP 484)
- Docstrings Google style
- Conforme PEP 8
- Gestion d'erreurs complÃ¨te

### Logging
- Logging structurÃ© avec Loguru
- Rotation automatique des fichiers de log
- Niveaux de log configurables
- Logs colorÃ©s dans la console

## ğŸ”§ API SimplifiÃ©e

### DNBScraper

```python
class DNBScraper:
    def __init__(self, base_url: str = BASE_URL, headless: bool = True)
    def extract_pdf_links(self, url: Optional[str] = None, max_pages: Optional[int] = None) -> List[Dict[str, str]]
    def get_summary_dict(self) -> Dict
    def close(self) -> None  # Ferme le WebDriver
    def extract_distinct_table_values(self) -> Dict[str, Set[str]]  # valeurs par page
    # AprÃ¨s extraction, les entrÃ©es structurÃ©es par ligne sont disponibles:
    # scraper.structured_entries: List[ExamEntry]
```

**Nouvelles fonctionnalitÃ©s :**
- Mode headless (navigateur invisible) par dÃ©faut
- Pagination automatique (parcourt toutes les pages avec JavaScript click)
- Fermeture automatique des overlays/modals
- Waits explicites pour une meilleure stabilitÃ©

### MetadataParser

```python
class MetadataParser:
    def __init__(self)
    def parse_url(self, url: str, data_atl_name: Optional[str] = None) -> Dict[str, Optional[str]]
```

### PDFDownloader

```python
class PDFDownloader:
    def __init__(self, output_dir: Path = RAW_DATA_DIR)
    def batch_download(self, urls: List[str], metadata: Optional[Dict] = None, 
                      max_workers: int = MAX_WORKERS, skip_existing: bool = True,
                      organize: bool = True) -> Dict[str, List]
```

## ğŸ› DÃ©pannage

### ChromeDriver not found

```bash
# VÃ©rifier que Chrome est installÃ©
# Windows: VÃ©rifier dans "Programmes et fonctionnalitÃ©s"
# Linux: google-chrome --version
# Mac: /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --version

# Selenium 4.15+ gÃ¨re ChromeDriver automatiquement
# Si problÃ¨me persiste, installez webdriver-manager:
pip install webdriver-manager
```

### Erreur "WebDriver initialization failed"

```bash
# VÃ©rifier la version de Chrome et Selenium
pip install --upgrade selenium

# Ou utiliser le mode visible pour dÃ©boguer
# Dans votre code: scraper = DNBScraper(headless=False)
```

### Timeout lors de la pagination

```bash
# Augmenter les timeouts dans .env
echo "SELENIUM_TIMEOUT=30" >> .env
echo "SELENIUM_PAGE_LOAD_WAIT=3.0" >> .env
```

### Erreur de connexion

```bash
# VÃ©rifier la connectivitÃ©
python -c "import requests; print(requests.get('https://eduscol.education.fr').status_code)"
```

### ProblÃ¨mes de SSL

```bash
# DÃ©sactiver la vÃ©rification SSL (non recommandÃ©)
echo "VERIFY_SSL=False" >> .env
```

### ImportError

```bash
# RÃ©installer les dÃ©pendances
pip install --force-reinstall -r requirements.txt
```

## ğŸ“ Exemples

### Obtenir des statistiques

```python
from src.scraper import DNBScraper

scraper = DNBScraper()
scraper.extract_pdf_links()
stats = scraper.get_summary_dict()

print(f"Total PDFs: {stats['total']}")
print(f"AnnÃ©es disponibles: {stats['years']}")
print(f"MatiÃ¨res disponibles: {stats['subjects']}")
print(f"Sujets: {stats['by_type']['sujet']}")
print(f"Corrections: {stats['by_type']['correction']}")
```

### TÃ©lÃ©charger uniquement les PDFs de 2024

```python
from src.scraper import DNBScraper
from src.downloader import PDFDownloader
from src.parser import MetadataParser

scraper = DNBScraper()
pdf_links = scraper.extract_pdf_links()

parser = MetadataParser()
links_2024 = []

for link_data in pdf_links:
    metadata = parser.parse_url(link_data['url'], link_data.get('data_atl_name'))
    if metadata['year'] == '2024':
        links_2024.append(link_data['url'])

print(f"TrouvÃ© {len(links_2024)} PDFs pour 2024")

downloader = PDFDownloader()
results = downloader.batch_download(links_2024)
```

## âš–ï¸ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Disclaimer

Ce projet est destinÃ© Ã  un usage Ã©ducatif et respecte les conditions d'utilisation du site eduscol.education.fr. Les utilisateurs sont responsables de l'utilisation qu'ils font de cet outil et doivent respecter les bonnes pratiques de web scraping Ã©thique.

## ğŸ‘¥ Auteurs

- **Sokratika** - *DÃ©veloppement initial* - [taieb1919](https://github.com/taieb1919)

## ğŸ™ Remerciements

- MinistÃ¨re de l'Ã‰ducation nationale pour la mise Ã  disposition des annales
- CommunautÃ© Python pour les excellentes bibliothÃ¨ques

## ğŸ“§ Contact

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub.

---

**Note** : Ce projet scrape des documents publics mis Ã  disposition par le ministÃ¨re de l'Ã‰ducation nationale franÃ§ais. Utilisez-le de maniÃ¨re responsable et respectueuse.
