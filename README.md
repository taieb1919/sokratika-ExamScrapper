# DNB Annales Scraper ğŸ“š

Un outil Python simple et efficace pour scraper et tÃ©lÃ©charger les annales du DiplÃ´me National du Brevet (DNB) depuis le site officiel du ministÃ¨re de l'Ã‰ducation nationale franÃ§ais.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ FonctionnalitÃ©s

- âœ… **Scraping avec Selenium** - Navigation automatique dans les pages avec JavaScript
- âœ… **Pagination automatique** - Parcourt toutes les pages
- âœ… **Extraction de liens** `/document/*/download` avec mÃ©tadonnÃ©es
- âœ… **TÃ©lÃ©chargement concurrent** avec gestion de threads
- âœ… **Organisation automatique** par annÃ©e et matiÃ¨re
- âœ… **Gestion d'erreurs robuste** avec retry automatique
- âœ… **Rate limiting** pour respecter le serveur
- âœ… **Logging complet** avec rotation de fichiers
- âœ… **Interface CLI** intuitive
- âœ… **Mode headless** ou visible pour le debugging

## ğŸ“‹ PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- **Google Chrome** (navigateur)
- **ChromeDriver** (compatible avec votre version de Chrome)
- Connexion Internet

### Installation de ChromeDriver

**MÃ©thode 1 - Automatique (recommandÃ©):**
ChromeDriver sera gÃ©rÃ© automatiquement par Selenium 4.15+

**MÃ©thode 2 - Manuelle:**
1. VÃ©rifiez votre version de Chrome : `chrome://version`
2. TÃ©lÃ©chargez ChromeDriver correspondant : https://chromedriver.chromium.org/downloads
3. Ajoutez ChromeDriver au PATH systÃ¨me ou placez-le dans le dossier du projet

## ğŸš€ Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/taieb1919/sokratika-ExamScrapper.git
cd sokratika-ExamScrapper
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**Note**: Selenium sera installÃ© automatiquement. Assurez-vous que Chrome est installÃ© sur votre systÃ¨me.

## ğŸ“– Utilisation

### Interface en ligne de commande (CLI)

Le projet fournit une interface CLI unifiÃ©e avec des options flexibles :

#### Utilisation de base

```bash
# Scraper et afficher le rÃ©sumÃ© (par dÃ©faut)
python main.py

# Limiter le nombre de pages parcourues (ex: 2 premiÃ¨res pages)
python main.py --pages 2
```

#### Validation des donnÃ©es

```bash
# Valider les entrÃ©es et gÃ©nÃ©rer validation_report.csv
python main.py --validate

# Valider en limitant Ã  2 pages
python main.py --validate --pages 2
```

#### TÃ©lÃ©chargement des PDFs

```bash
# TÃ©lÃ©charger tous les PDFs aprÃ¨s scraping
python main.py --download

# TÃ©lÃ©charger en limitant Ã  2 pages
python main.py --download --pages 2

# Valider d'abord, puis tÃ©lÃ©charger seulement si validation OK
python main.py --validate --download
```

#### Options avancÃ©es

```bash
# Personnaliser l'URL de scraping
python main.py --url "https://custom-url.com" --download

# Changer le niveau de logging
python main.py --log-level DEBUG --download

# SpÃ©cifier un fichier de log personnalisÃ©
python main.py --log-file custom.log --download
```

### Utilisation programmatique

```python
from src.scraper import DNBScraper
from src.downloader import PDFDownloader

# Initialisation du scraper (headless par dÃ©faut)
scraper = DNBScraper()

# Ou en mode visible pour debugging
# scraper = DNBScraper(headless=False)

# Extraction des liens PDF (toutes les pages automatiquement)
pdf_links = scraper.extract_pdf_links()
print(f"TrouvÃ© {len(pdf_links)} PDFs sur toutes les pages")

# Obtenir le rÃ©sumÃ©
summary = scraper.get_summary_dict()
print(f"AnnÃ©es disponibles: {summary['years']}")
print(f"MatiÃ¨res disponibles: {summary['subjects']}")

# AccÃ©der aux entrÃ©es structurÃ©es (nouvelle approche)
entries = scraper.structured_entries
print(f"EntrÃ©es structurÃ©es: {len(entries)}")

for entry in entries[:3]:  # Premiers 3 exemples
    print(f"Session: {entry.session.value}")
    print(f"Discipline: {entry.discipline.value}")
    print(f"SÃ©rie: {entry.serie.value}")
    print(f"Localisation: {entry.localisation.value}")
    print(f"Fichiers: {len(entry.files)}")
    for f in entry.files:
        print(f"  - {f.filename_for_save}.pdf")
    print()

# TÃ©lÃ©chargement avec mÃ©tadonnÃ©es structurÃ©es
urls = []
metadata_list = []
for entry in entries:
    for f in entry.files:
        urls.append(f.download_url)
        metadata_list.append({
            'url': f.download_url,
            'filename': f.filename_for_save + '.pdf',
            'file_id': f.file_id,
            'year': entry.session.value.split('_')[0] if '_' in entry.session.value else None,
            'subject': entry.discipline.value,
            'session': entry.session.value,
            'series': entry.serie.value,
            'is_correction': False,
            'document_type': 'sujet',
        })

downloader = PDFDownloader(output_dir="data/raw")
results = downloader.batch_download(
    urls=urls,
    metadata={'all': metadata_list},
    max_workers=5
)

print(f"TÃ©lÃ©chargÃ©s: {len(results['successful'])}")
print(f"Ã‰checs: {len(results['failed'])}")
```

## ğŸ—‚ï¸ Structure du projet

```
sokratika-ExamScrapper/
â”‚
â”œâ”€â”€ src/                        # Code source principal
â”‚   â”œâ”€â”€ __init__.py            # Initialisation du package
â”‚   â”œâ”€â”€ scraper.py             # Module de scraping (DNBScraper)
â”‚   â”œâ”€â”€ downloader.py          # Module de tÃ©lÃ©chargement (PDFDownloader)
â”‚   â”œâ”€â”€ parser.py              # Analyseur de mÃ©tadonnÃ©es (MetadataParser)
â”‚   â””â”€â”€ utils.py               # Fonctions utilitaires
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â””â”€â”€ settings.py            # ParamÃ¨tres de configuration
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es tÃ©lÃ©chargÃ©es
â”‚   â”œâ”€â”€ raw/                   # PDFs tÃ©lÃ©chargÃ©s (organisÃ©s par annÃ©e/matiÃ¨re)
â”‚   â””â”€â”€ metadata/              # Fichiers JSON avec mÃ©tadonnÃ©es
â”‚
â”œâ”€â”€ logs/                      # Fichiers de logs
â”‚
â”œâ”€â”€ tests/                     # Tests (vide dans version simplifiÃ©e)
â”‚
â”œâ”€â”€ main.py                    # Point d'entrÃ©e CLI
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ setup.py                   # Configuration du package
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸ“Š Structure HTML du site

Le scraper utilise **Selenium WebDriver** pour gÃ©rer la pagination JavaScript et extraire les liens de toutes les pages.

### Pagination :
- **Boutons de navigation** : Premier, PrÃ©cÃ©dent, [1][2][3]..., Suivant, Dernier
- **Navigation automatique** : Parcourt toutes les pages jusqu'Ã  la derniÃ¨re avec le bouton "Suivant"
- **JavaScript click** : Utilise JavaScript pour Ã©viter les interceptions d'Ã©lÃ©ments

### Structure du tableau :

```html
<table>
  <tbody>
    <tr>
      <td>Session</td>
      <td>Discipline</td>
      <td>SÃ©rie</td>
      <td>Localisation</td>
      <td class="views-field-link">
        <a href="/document/[ID]/download" data-atl-name="24genfrdag1_v11.pdf|63414">
          TÃ©lÃ©charger
        </a>
      </td>
    </tr>
  </tbody>
</table>
```

### Ã‰lÃ©ments clÃ©s :
- **Liens** : Format `/document/[ID]/download` (pas `.pdf`)
- **MÃ©tadonnÃ©es** : Attribut `data-atl-name` contenant `"filename.pdf|file_id"`
- **Colonnes** : Session, Discipline, SÃ©rie, Localisation, Liens
- **Pagination** : GÃ©rÃ©e dynamiquement avec JavaScript

## ğŸ“Š MÃ©tadonnÃ©es extraites

Le parser extrait automatiquement les informations depuis `data-atl-name` et les noms de fichiers :

- **AnnÃ©e** : AnnÃ©e de l'examen (ex: "2024" depuis "24genfrdag1_v11.pdf")
- **MatiÃ¨re** : MathÃ©matiques, FranÃ§ais, Histoire-GÃ©ographie, Sciences, etc.
- **Session** : normale, remplacement
- **SÃ©rie** : gÃ©nÃ©rale, professionnelle
- **Type de document** : sujet ou correction
- **URL** : Lien de tÃ©lÃ©chargement
- **Nom de fichier** : Nom extrait du data-atl-name
- **ID du fichier** : ID numÃ©rique du document

### Exemple de parsing :

```python
# data-atl-name: "24genfrdag1_v11.pdf|63414"
{
    'filename': '24genfrdag1_v11.pdf',
    'file_id': '63414',
    'year': '2024',           # "24" â†’ "2024"
    'subject': 'FranÃ§ais',    # "fr" dÃ©tectÃ©
    'series': 'generale',     # "gen" dÃ©tectÃ©
    'document_type': 'sujet',
    'is_correction': False,
    'url': 'https://eduscol.education.fr/document/123/download'
}
```

## âš™ï¸ Configuration

### Variables d'environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Selenium settings
SELENIUM_HEADLESS=True         # Mode headless (True) ou visible (False)
SELENIUM_TIMEOUT=20            # Timeout pour les Ã©lÃ©ments (secondes)
SELENIUM_PAGE_LOAD_WAIT=2.0    # Attente entre les pages (secondes)

# Rate limiting
REQUEST_DELAY=1.5              # DÃ©lai entre les requÃªtes (secondes)
DOWNLOAD_TIMEOUT=30            # Timeout de tÃ©lÃ©chargement (secondes)
MAX_RETRIES=3                  # Nombre de tentatives en cas d'Ã©chec

# Download settings
MAX_WORKERS=5                  # Nombre de tÃ©lÃ©chargements concurrents
VERIFY_SSL=True                # VÃ©rifier les certificats SSL

# Organization
ORGANIZE_BY_YEAR=True          # Organiser par annÃ©e
ORGANIZE_BY_SUBJECT=True       # Organiser par matiÃ¨re

# Logging
LOG_LEVEL=INFO                 # Niveau de log (DEBUG, INFO, WARNING, ERROR)
```

### Personnalisation dans `config/settings.py`

Vous pouvez modifier les paramÃ¨tres directement dans `config/settings.py` :

- URL de scraping
- Headers HTTP
- Mapping des matiÃ¨res
- Chemins des fichiers

## ğŸ“ Organisation des fichiers

Les PDFs tÃ©lÃ©chargÃ©s sont automatiquement organisÃ©s :

```
data/raw/
â”œâ”€â”€ 2024/
â”‚   â”œâ”€â”€ FranÃ§ais/
â”‚   â”‚   â”œâ”€â”€ 24genfrdag1_v11.pdf
â”‚   â”‚   â””â”€â”€ 24genfrdag1_corrige.pdf
â”‚   â”œâ”€â”€ MathÃ©matiques/
â”‚   â”‚   â”œâ”€â”€ 24genmathag1_v11.pdf
â”‚   â”‚   â””â”€â”€ 24genmathag1_corrige.pdf
â”‚   â””â”€â”€ Histoire-GÃ©ographie/
â”‚       â””â”€â”€ 24genhisgeoag1_v11.pdf
â””â”€â”€ 2023/
    â””â”€â”€ ...
```

## ğŸ›¡ï¸ Bonnes pratiques implÃ©mentÃ©es

### Respect du serveur
- Rate limiting (dÃ©lai configurable entre requÃªtes)
- User-Agent appropriÃ©
- Gestion des erreurs HTTP
- Retry avec backoff exponentiel

### QualitÃ© du code
- Type hints (PEP 484)
- Docstrings Google style
- Conforme PEP 8
- Gestion d'erreurs complÃ¨te

### Logging
- Logging structurÃ© avec Loguru
- Rotation automatique des fichiers de log
- Niveaux de log configurables
- Logs colorÃ©s dans la console

## ğŸ”§ API SimplifiÃ©e

### DNBScraper

```python
class DNBScraper:
    def __init__(self, base_url: str = BASE_URL, headless: bool = True)
    def extract_pdf_links(self, url: Optional[str] = None, max_pages: Optional[int] = None) -> List[Dict[str, str]]
    def get_summary_dict(self) -> Dict
    def close(self) -> None  # Ferme le WebDriver
    def extract_distinct_table_values(self) -> Dict[str, Set[str]]  # valeurs par page
    
    # PropriÃ©tÃ©s disponibles aprÃ¨s extraction:
    structured_entries: List[ExamEntry]  # EntrÃ©es structurÃ©es par ligne
```

**FonctionnalitÃ©s principales :**
- Mode headless (navigateur invisible) par dÃ©faut
- Pagination automatique (parcourt toutes les pages avec JavaScript click)
- Fermeture automatique des overlays/modals
- Waits explicites pour une meilleure stabilitÃ©
- Extraction structurÃ©e des donnÃ©es en objets `ExamEntry`

### PDFDownloader

```python
class PDFDownloader:
    def __init__(self, output_dir: Path = RAW_DATA_DIR)
    def batch_download(self, urls: List[str], metadata: Optional[Dict] = None, 
                      max_workers: int = MAX_WORKERS, skip_existing: bool = True,
                      organize: bool = True) -> Dict[str, List]
    def print_statistics(self) -> None
    def save_metadata(self) -> None
    def save_failed_downloads(self) -> None
```

### ModÃ¨les de donnÃ©es

```python
@dataclass
class ExamEntry:
    id: int
    session: SessionEnum
    discipline: DisciplineEnum
    serie: SerieEnum
    localisation: LocalisationEnum
    files: List[ExamFile]

@dataclass
class ExamFile:
    file_id: str
    filename: str
    filename_for_save: str
    download_url: str
```

## ğŸ› DÃ©pannage

### ChromeDriver not found

```bash
# VÃ©rifier que Chrome est installÃ©
# Windows: VÃ©rifier dans "Programmes et fonctionnalitÃ©s"
# Linux: google-chrome --version
# Mac: /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --version

# Selenium 4.15+ gÃ¨re ChromeDriver automatiquement
# Si problÃ¨me persiste, installez webdriver-manager:
pip install webdriver-manager
```

### Erreur "WebDriver initialization failed"

```bash
# VÃ©rifier la version de Chrome et Selenium
pip install --upgrade selenium

# Ou utiliser le mode visible pour dÃ©boguer
# Dans votre code: scraper = DNBScraper(headless=False)
```

### Timeout lors de la pagination

```bash
# Augmenter les timeouts dans .env
echo "SELENIUM_TIMEOUT=30" >> .env
echo "SELENIUM_PAGE_LOAD_WAIT=3.0" >> .env
```

### Erreur de connexion

```bash
# VÃ©rifier la connectivitÃ©
python -c "import requests; print(requests.get('https://eduscol.education.fr').status_code)"
```

### ProblÃ¨mes de SSL

```bash
# DÃ©sactiver la vÃ©rification SSL (non recommandÃ©)
echo "VERIFY_SSL=False" >> .env
```

### ImportError

```bash
# RÃ©installer les dÃ©pendances
pip install --force-reinstall -r requirements.txt
```

## ğŸ“ Exemples

### Obtenir des statistiques

```python
from src.scraper import DNBScraper

scraper = DNBScraper()
scraper.extract_pdf_links()
stats = scraper.get_summary_dict()

print(f"Total PDFs: {stats['total']}")
print(f"AnnÃ©es disponibles: {stats['years']}")
print(f"MatiÃ¨res disponibles: {stats['subjects']}")
print(f"Sujets: {stats['by_type']['sujet']}")
print(f"Corrections: {stats['by_type']['correction']}")
```

### TÃ©lÃ©charger uniquement les PDFs de 2024

```python
from src.scraper import DNBScraper
from src.downloader import PDFDownloader

scraper = DNBScraper()
scraper.extract_pdf_links()

# Filtrer les entrÃ©es de 2024
entries_2024 = []
for entry in scraper.structured_entries:
    if entry.session.value.startswith('2024'):
        entries_2024.append(entry)

print(f"TrouvÃ© {len(entries_2024)} entrÃ©es pour 2024")

# PrÃ©parer les URLs et mÃ©tadonnÃ©es
urls_2024 = []
metadata_list_2024 = []
for entry in entries_2024:
    for f in entry.files:
        urls_2024.append(f.download_url)
        metadata_list_2024.append({
            'url': f.download_url,
            'filename': f.filename_for_save + '.pdf',
            'file_id': f.file_id,
            'year': '2024',
            'subject': entry.discipline.value,
            'session': entry.session.value,
            'series': entry.serie.value,
            'is_correction': False,
            'document_type': 'sujet',
        })

downloader = PDFDownloader()
results = downloader.batch_download(
    urls=urls_2024,
    metadata={'all': metadata_list_2024}
)
```

## âš–ï¸ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Disclaimer

Ce projet est destinÃ© Ã  un usage Ã©ducatif et respecte les conditions d'utilisation du site eduscol.education.fr. Les utilisateurs sont responsables de l'utilisation qu'ils font de cet outil et doivent respecter les bonnes pratiques de web scraping Ã©thique.

## ğŸ‘¥ Auteurs

- **Sokratika** - *DÃ©veloppement initial* - [taieb1919](https://github.com/taieb1919)

## ğŸ™ Remerciements

- MinistÃ¨re de l'Ã‰ducation nationale pour la mise Ã  disposition des annales
- CommunautÃ© Python pour les excellentes bibliothÃ¨ques

## ğŸ“§ Contact

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub.

---

**Note** : Ce projet scrape des documents publics mis Ã  disposition par le ministÃ¨re de l'Ã‰ducation nationale franÃ§ais. Utilisez-le de maniÃ¨re responsable et respectueuse.
